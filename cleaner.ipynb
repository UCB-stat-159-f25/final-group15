{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee930ee0-3cca-4f97-a44d-df8e534b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE:\n",
    "#there is no 04/2020 for the bike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f47dfc05-fa16-44a3-b10b-5c9da4b2d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(char=\"=\", length=50):\n",
    "    return char * length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "608319ae-7a14-4ed4-b80b-7f4a779f10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c939fa21-d484-4c6f-9e4d-08e5771bdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_aggregate_daily_stats(data_folder=\"data2\"):\n",
    "    zip_files = sorted(glob.glob(f\"{data_folder}/*.zip\"))\n",
    "    print(line())\n",
    "    print(f\"Processing {len(zip_files)} files...\")\n",
    "    print(line())\n",
    "    daily_sums = {}\n",
    "    for i, zf in enumerate(zip_files, start=1):\n",
    "        print(f\"[{i}/{len(zip_files)}] {zf}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zf, 'r') as z:\n",
    "            csv_name = z.namelist()[0]\n",
    "            with z.open(csv_name) as f:\n",
    "                for chunk in pd.read_csv(f, chunksize=50000, low_memory=False):\n",
    "                    # Find time column\n",
    "                    time_col = next((c for c in ['start_time', 'started_at', 'Start Time'] if c in chunk.columns), None)\n",
    "                    if not time_col:\n",
    "                        continue\n",
    "                    # Find duration column\n",
    "                    dur_col = next((c for c in ['duration_sec', 'Duration', 'tripduration']if c in chunk.columns), None)\n",
    "                    \n",
    "                    # Clean dates\n",
    "                    chunk[time_col] = pd.to_datetime(chunk[time_col], errors='coerce')\n",
    "                    chunk = chunk.dropna(subset=[time_col])\n",
    "                    chunk['date'] = chunk[time_col].dt.date\n",
    "            \n",
    "                    # Aggregate by date\n",
    "                    for date, grp in chunk.groupby('date'):\n",
    "                        if date not in daily_sums:\n",
    "                            daily_sums[date] = {'dur': 0, 'trips': 0, 'bike_share': 0, 'subs': 0}\n",
    "                        \n",
    "                        if dur_col:\n",
    "                            daily_sums[date]['dur'] += grp[dur_col].sum()\n",
    "                        daily_sums[date]['trips'] += len(grp)\n",
    "                        if 'bike_share_for_all_trip' in grp.columns:\n",
    "                            daily_sums[date]['bike_share'] += (grp['bike_share_for_all_trip'] == 'Yes').sum()\n",
    "                        if 'user_type' in grp.columns:\n",
    "                            daily_sums[date]['subs'] += (grp['user_type'] == 'Subscriber').sum()\n",
    "        if i % 10 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    rows = []\n",
    "    for date, s in daily_sums.items():\n",
    "        rows.append({\n",
    "            'date': date,\n",
    "            'avg_duration_sec': s['dur'] / s['trips'],\n",
    "            'pct_bike_share_yes': (s['bike_share'] / s['trips']) * 100,\n",
    "            'pct_subscriber': (s['subs'] / s['trips']) * 100,\n",
    "            'total_trips': s['trips']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows).sort_values('date').set_index('date')\n",
    "    print(f\"\\nDone: {len(df)} days, {df['total_trips'].sum():,.0f} trips\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8338dfa6-edea-4b95-ae8a-4618ef696c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing 83 files...\n",
      "==================================================\n",
      "[1/83] data2/201801-fordgobike-tripdata.csv.zip\n",
      "[2/83] data2/201802-fordgobike-tripdata.csv.zip\n",
      "[3/83] data2/201803-fordgobike-tripdata.csv.zip\n",
      "[4/83] data2/201804-fordgobike-tripdata.csv.zip\n",
      "[5/83] data2/201805-fordgobike-tripdata.csv.zip\n",
      "[6/83] data2/201806-fordgobike-tripdata.csv.zip\n",
      "[7/83] data2/201807-fordgobike-tripdata.csv.zip\n",
      "[8/83] data2/201808-fordgobike-tripdata.csv.zip\n",
      "[9/83] data2/201809-fordgobike-tripdata.csv.zip\n",
      "[10/83] data2/201810-fordgobike-tripdata.csv.zip\n",
      "[11/83] data2/201811-fordgobike-tripdata.csv.zip\n",
      "[12/83] data2/201812-fordgobike-tripdata.csv.zip\n",
      "[13/83] data2/201901-fordgobike-tripdata.csv.zip\n",
      "[14/83] data2/201902-fordgobike-tripdata.csv.zip\n",
      "[15/83] data2/201903-fordgobike-tripdata.csv.zip\n",
      "[16/83] data2/201904-fordgobike-tripdata.csv.zip\n",
      "[17/83] data2/201905-baywheels-tripdata.csv.zip\n",
      "[18/83] data2/201906-baywheels-tripdata.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_171/1439153565.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[time_col] = pd.to_datetime(chunk[time_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/83] data2/201907-baywheels-tripdata.csv.zip\n",
      "[20/83] data2/201908-baywheels-tripdata.csv.zip\n",
      "[21/83] data2/201909-baywheels-tripdata.csv.zip\n",
      "[22/83] data2/201910-baywheels-tripdata.csv.zip\n",
      "[23/83] data2/201911-baywheels-tripdata.csv.zip\n",
      "[24/83] data2/201912-baywheels-tripdata.csv.zip\n",
      "[25/83] data2/202001-baywheels-tripdata.csv.zip\n",
      "[26/83] data2/202002-baywheels-tripdata.csv.zip\n",
      "[27/83] data2/202003-baywheels-tripdata.csv.zip\n",
      "[28/83] data2/202005-baywheels-tripdata.csv.zip\n",
      "[29/83] data2/202006-baywheels-tripdata.csv.zip\n",
      "[30/83] data2/202007-baywheels-tripdata.csv.zip\n",
      "[31/83] data2/202008-baywheels-tripdata.csv.zip\n",
      "[32/83] data2/202009-baywheels-tripdata.csv.zip\n",
      "[33/83] data2/202010-baywheels-tripdata.csv.zip\n",
      "[34/83] data2/202011-baywheels-tripdata.csv.zip\n",
      "[35/83] data2/202012-baywheels-tripdata.csv.zip\n",
      "[36/83] data2/202101-baywheels-tripdata.csv.zip\n",
      "[37/83] data2/202102-baywheels-tripdata.csv.zip\n",
      "[38/83] data2/202103-baywheels-tripdata.csv.zip\n",
      "[39/83] data2/202104-baywheels-tripdata.csv.zip\n",
      "[40/83] data2/202105-baywheels-tripdata.csv.zip\n",
      "[41/83] data2/202106-baywheels-tripdata.csv.zip\n",
      "[42/83] data2/202107-baywheels-tripdata.csv.zip\n",
      "[43/83] data2/202108-baywheels-tripdata.csv.zip\n",
      "[44/83] data2/202109-baywheels-tripdata.csv.zip\n",
      "[45/83] data2/202110-baywheels-tripdata.csv.zip\n",
      "[46/83] data2/202111-baywheels-tripdata.csv.zip\n",
      "[47/83] data2/202112-baywheels-tripdata.csv.zip\n",
      "[48/83] data2/202201-baywheels-tripdata.csv.zip\n",
      "[49/83] data2/202202-baywheels-tripdata.csv.zip\n",
      "[50/83] data2/202203-baywheels-tripdata.csv.zip\n",
      "[51/83] data2/202204-baywheels-tripdata.csv.zip\n",
      "[52/83] data2/202205-baywheels-tripdata.csv.zip\n",
      "[53/83] data2/202206-baywheels-tripdata.csv.zip\n",
      "[54/83] data2/202207-baywheels-tripdata.csv.zip\n",
      "[55/83] data2/202208-baywheels-tripdata.csv.zip\n",
      "[56/83] data2/202209-baywheels-tripdata.csv.zip\n",
      "[57/83] data2/202210-baywheels-tripdata.csv.zip\n",
      "[58/83] data2/202211-baywheeels-tripdata.csv.zip\n",
      "[59/83] data2/202212-baywheels-tripdata.csv.zip\n",
      "[60/83] data2/202301-baywheels-tripdata.csv.zip\n",
      "[61/83] data2/202302-baywheels-tripdata.csv.zip\n",
      "[62/83] data2/202303-baywheels-tripdata.csv.zip\n",
      "[63/83] data2/202304-baywheels-tripdata.csv.zip\n",
      "[64/83] data2/202305-baywheels-tripdata.csv.zip\n",
      "[65/83] data2/202306-baywheels-tripdata.csv.zip\n",
      "[66/83] data2/202307-baywheels-tripdata.csv.zip\n",
      "[67/83] data2/202308-baywheels-tripdata.csv.zip\n",
      "[68/83] data2/202309-baywheels-tripdata.csv.zip\n",
      "[69/83] data2/202310-baywheels-tripdata.csv.zip\n",
      "[70/83] data2/202311-baywheels-tripdata.csv.zip\n",
      "[71/83] data2/202312-baywheels-tripdata.csv.zip\n",
      "[72/83] data2/202401-baywheels-tripdata.csv.zip\n",
      "[73/83] data2/202402-baywheels-tripdata.csv.zip\n",
      "[74/83] data2/202403-baywheels-tripdata.csv.zip\n",
      "[75/83] data2/202404-baywheels-tripdata.csv.zip\n",
      "[76/83] data2/202405-baywheels-tripdata.csv.zip\n",
      "[77/83] data2/202406-baywheels-tripdata.csv.zip\n",
      "[78/83] data2/202407-baywheels-tripdata.csv.zip\n",
      "[79/83] data2/202408-baywheels-tripdata.csv.zip\n",
      "[80/83] data2/202409-baywheels-tripdata.csv.zip\n",
      "[81/83] data2/202410-baywheels-tripdata.csv.zip\n",
      "[82/83] data2/202411-baywheels-tripdata.csv.zip\n",
      "[83/83] data2/202412-baywheels-tripdata.csv.zip\n",
      "\n",
      "Done: 2528 days, 16,816,499 trips\n",
      "            avg_duration_sec  pct_bike_share_yes  pct_subscriber  total_trips\n",
      "date                                                                         \n",
      "2018-01-01       1870.569455            9.818182       57.818182         1375\n",
      "2018-01-02        979.447417            5.965560       88.407134         3252\n",
      "2018-01-03        739.360168            4.865243       91.774589         2857\n",
      "2018-01-04        730.779091            5.000000       91.606061         3300\n",
      "2018-01-05        756.418605            5.953488       88.930233         2150\n"
     ]
    }
   ],
   "source": [
    "daily_stats = load_and_aggregate_daily_stats()\n",
    "daily_stats.to_csv('daily_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e31b5f86-9c4d-49c8-a67e-0b8f7a867e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 38,326\n",
      "Filtered rows: 2,557\n",
      "Date range: 2018-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "✓ Exported to: weather_2018_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_171/3037975613.py:4: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"USW00023272.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load any NOAA CSV of type\n",
    "# https://www.ncei.noaa.gov/access/search/data-search/daily-summaries?bbox=38.075,-122.715,37.485,-122.125&startDate=2018-01-01T00:00:00&endDate=2024-12-31T23:59:59&pageNum=1\n",
    "\n",
    "df = pd.read_csv(\"USW00023272.csv\")\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "mask = (df['DATE'] >= '2018-01-01') & (df['DATE'] <= '2024-12-31')\n",
    "df_filtered = df[mask]\n",
    "\n",
    "print(f\"Original rows: {len(df):,}\")\n",
    "print(f\"Filtered rows: {len(df_filtered):,}\")\n",
    "print(f\"Date range: {df_filtered['DATE'].min()} to {df_filtered['DATE'].max()}\")\n",
    "\n",
    "output_file = \"weather_2018_2024.csv\"\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "print(f\"✓ Exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25906b-2f1b-4b26-abbc-2e88bd8225c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
